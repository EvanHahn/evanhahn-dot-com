<html><head><title>EECS 203 exam #2 study guide</title><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=76EbY9A2YqtjL6RumUs0pOJuDkyNmpd_vXS4DtNvCus');ol{margin:0;padding:0}p{margin:0}.c3{list-style-type:square;margin:0;padding:0}.c16{max-width:468pt;background-color:#ffffff;padding:36pt 72pt 36pt 72pt}.c1{padding-left:0pt;direction:ltr;margin-left:72pt}.c8{list-style-type:disc;margin:0;padding:0}.c0{padding-left:0pt;direction:ltr;margin-left:36pt}.c2{list-style-type:circle;margin:0;padding:0}.c6{color:#1155cc;text-decoration:underline}.c20{color:#333333;background-color:#ffffff}.c14{padding-left:0pt;margin-left:108pt}.c10{color:inherit;text-decoration:inherit}.c7{height:11pt;direction:ltr}.c17{font-size:18pt}.c4{font-style:italic}.c12{vertical-align:super}.c15{text-decoration:underline}.c19{background-color:#ffff00}.c21{background-color:#ffffff}.c11{vertical-align:sub}.c13{text-align:center}.c5{font-weight:bold}.c9{direction:ltr}.c18{font-size:24pt}.title{padding-top:24pt;line-height:1.15;text-align:left;color:#000000;font-size:36pt;font-family:Arial;font-weight:bold;padding-bottom:6pt}.subtitle{padding-top:18pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:24pt;font-family:Georgia;padding-bottom:4pt}body{color:#000000;font-size:11pt;font-family:Arial}h1{padding-top:24pt;line-height:1.15;text-align:left;color:#000000;font-size:18pt;font-family:Arial;font-weight:bold;padding-bottom:6pt}h2{padding-top:18pt;line-height:1.15;text-align:left;color:#000000;font-size:14pt;font-family:Arial;font-weight:bold;padding-bottom:4pt}h3{padding-top:14pt;line-height:1.15;text-align:left;color:#666666;font-size:12pt;font-family:Arial;font-weight:bold;padding-bottom:4pt}h4{padding-top:12pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:11pt;font-family:Arial;padding-bottom:2pt}h5{padding-top:11pt;line-height:1.15;text-align:left;color:#666666;font-size:10pt;font-family:Arial;font-weight:bold;padding-bottom:2pt}h6{padding-top:10pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:10pt;font-family:Arial;padding-bottom:2pt}</style></head><body class="c16"><p class="c13 c9 title"><a name="h.688djcfxvfzm"></a><span class="c18">EECS 203 exam #2 study guide</span></p><p class="c13 c9 subtitle"><a name="h.uwtek95e2hn8"></a><span class="c17">Winter 2012, University of Michigan</span></p><p class="c7"><span></span></p><p class="c13 c9"><span>by Evan Hahn + Scott Godbold + Brad Hekman + Alex Ihlenburg (add your names here if you helped!) + Ryan Yezman + Matt Schulte</span></p><p class="c7 c13"><span></span></p><p class="c13 c9"><span>This study guide is free-license in the public domain. Its accuracy is not guaranteed.</span></p><p class="c7 c13"><span></span></p><p class="c13 c9"><span>Feel free to edit if you think you have something to add, but please do not ruin this beautiful document. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c7"><span></span></p><hr><p class="c7"><span></span></p><h1 class="c9"><a name="h.np78hoi0xh9d"></a><span>Exam info</span></h1><ol class="c8" start="1"><li class="c0"><span>Covers lectures 11 - 18, Stirling numbers, homework 6-8 (and first half of 9), Rosen 3.1 - 3.3, 6.1 - 6.5, and 7.1 - 7.4</span></li><li class="c0"><span>You can bring 2 single-sided sheets (aka one double-sided sheet)</span></li><li class="c0"><span>&quot;In addition, in your solutions you are allowed to quote without proof any results proved in the lectures, discussion sections, homework, the sample exam, and the textbook.&quot;</span></li></ol><p class="c7"><span></span></p><hr><p class="c7"><span></span></p><h1 class="c9"><a name="h.vhto4le7tnyu"></a><span>Quick reference (blatantly copied and edited from </span><span class="c6"><a class="c10" href="https://docs.google.com/a/evanhahn.com/document/d/1F-tKiuFBBBhBPd1NSKfCRArxBeyot4MItRF4A--a3Ik/edit">this awesomeness</a></span><span>)</span></h1><ol class="c8" start="1"><li class="c0"><span class="c5">algorithm</span><span>&nbsp;- a finite sequence for precise instructions for performing a computation or solving a problem</span></li><li class="c0"><span class="c5">searching algorithm</span><span>&nbsp;- the problem of locating an element in a list</span></li><li class="c0"><span class="c5">linear search algorithm</span><span>&nbsp;- a procedure for searching a list element by element. the linear search has O(n) wctc.</span></li><li class="c0"><span class="c5">binary search algorithm</span><span>&nbsp;- a procedure for searching an ordered list by successively splitting the list in half. the binary search has O(log n) wctc</span></li><li class="c0"><span class="c5">sorting</span><span>&nbsp;- the reordering of the elements of a list into prescribed order</span></li><li class="c0"><span class="c5">f(x) is O(g(x))</span><span>&nbsp;- the fact that |f(x)| &nbsp;&lt;= C|g(x)| for all k &gt; x for some constants C and k</span></li><li class="c0"><span class="c5">witness to the relationship f(x) is O(g(x))</span><span>&nbsp;- a pair C and k such that |f(x)| &nbsp;&lt;= C|g(x)| for all k &gt; x</span></li><li class="c0"><span class="c5">f(x) is big omega(g(x))</span><span>&nbsp;- the fact that |f(x)| &gt;= C|g(x)| for all x &gt; k for some positive constants C and k</span></li><li class="c0"><span class="c5">f(x) is big theta(g(x))</span><span>&nbsp;- the fact that f(x) is big O and big omega of g(x)</span></li><li class="c0"><span class="c5">time complexity</span><span>&nbsp;- the amount of time required for an algorithm to solve a problem</span></li><li class="c0"><span class="c5">space complexity</span><span>&nbsp;- the amount of space in a computer memory required for an algorithm to solve a problem</span></li><li class="c0"><span class="c5">worst-case time complexity (WCTC)</span><span>&nbsp;- the greatest amount of time required for an algorithm to solve a problem of a given size</span></li><li class="c0"><span class="c5">average case time complexity</span><span>&nbsp;- the average amount of time required for an algorithm to solve a problem of a given size</span></li><li class="c0"><span class="c5">algorithmic paradigm</span><span>&nbsp;- a general approach for constructing algorithms based on a particular concept</span></li><li class="c0"><span class="c5">greedy algorithm</span><span>&nbsp;- an algorithm that makes the best choice at each step according to some specified condition</span></li><li class="c0"><span class="c5">tractable problem</span><span>&nbsp;- a problem for which there is a worst case polynomial time algorithm that solves it</span></li><li class="c0"><span class="c5">intractable problem</span><span>&nbsp;- a problem for which no worst case polynomial time algorithm exists for solving it</span></li><li class="c0"><span class="c5">bubble sort </span><span>- sorting algorithm that uses passes where successive items are interchanged if they are in the wrong order. bubble and insertion sort have O(n^2) wctc</span></li><li class="c0"><span class="c5">insertion sort</span><span>&nbsp;- a sorting that at the jth step inserts the jth element into the correct position in the list, when the first j-1 elements of the list are already ordered. bubble and insertion sort have O(n^2) wctc</span></li><li class="c0"><span class="c5">combinatorics</span><span>&nbsp;-</span><span>&nbsp;the study of arrangements of objects</span></li><li class="c0"><span class="c5">enumerations</span><span>&nbsp;- the counting of arrangements of objects</span></li><li class="c0"><span class="c5">permutation</span><span>&nbsp;- an ordered arrangement of the elements of a set</span></li><li class="c0"><span class="c5">r-permutation</span><span>&nbsp;- an ordered arrangement of r elements of a set</span></li><li class="c0"><span class="c5">P(n,r)</span><span>&nbsp;- the number of r-permutations of a set with n elements</span></li><li class="c0"><span class="c5">r-combination</span><span>&nbsp;- an unordered selection of r elements of a set</span></li><li class="c0"><span class="c5">C(n,r)</span><span>&nbsp;- the number of r-combinations of a set with n elements</span></li><li class="c0"><span class="c5">binomial coefficient (n over r)</span><span>&nbsp;- also the number of r-combinations of a set with n elements</span></li><li class="c0"><span class="c5">combinatorial proof</span><span>&nbsp;- a proof that uses counting arguments rather than algebraic manipulation to prove a result</span></li><li class="c0"><span class="c5">pascal&rsquo;s triangle</span><span>&nbsp;- a representation of the binomial coefficients where the ith row of the triangle contains (I over j) for j = 0, 1, 2, &hellip;</span></li><li class="c0"><span class="c5">S(n,j)</span><span>&nbsp;- the Stirling number of the second kind denoting the number of ways to distribute n distinguishable objects into j indistinguishable boxes so that no box is empty</span></li><li class="c0"><span class="c5">product rule for counting</span><span>: the number of ways to do a procedure that consists of two tasks is the product of the number of ways to do the first tasks and the number of ways to do the second task</span></li><li class="c0"><span class="c5">product rule for sets</span><span>&nbsp;- the number of elements in the Cartesian product of finite sets is the product of the number of elements in each set</span></li><li class="c0"><span class="c5">sum rule for counting</span><span>&nbsp;- the number of ways to do a task in one of two ways is the sum of the number of ways to do these tasks if they cannot be done simultaneously</span></li><li class="c0"><span class="c5">sum rule for sets</span><span>&nbsp;- the number of elements in the union of pairwise disjoint finite sets is the sum of the numbers of elements in these sets</span></li><li class="c0"><span class="c5">subtraction rule for counting or inclusion-exclusion for sets</span><span>&nbsp;- if a task can be done in either n1 or n2 ways, then the number of ways to do the talk in n1+n2 minus the number of ways to do the talk that are common to the two different ways</span></li><li class="c0"><span class="c5">subtraction rule or inclusion exclusion for sets</span><span>&nbsp;- the number of elements in the union of two sets is the sum of the number of elements in these sets minus the number of elements in their intersection</span></li><li class="c0"><span class="c5">division rule for counting</span><span>&nbsp;- there are n/d ways to do a task if it can be done using a procedure that can be carried out in n ways, and for every way w, exactly n of the n ways correspond to way w</span></li><li class="c0"><span class="c5">division rule for sets</span><span>&nbsp;- suppose that a finite set A is the union of n disjoint subsets with d elements. then n = |A|/d</span></li><li class="c0"><span class="c5">the pigeonhole principle</span><span>&nbsp;- when more than k objects are placed in k boxes, there must be a box containing more than one object</span></li><li class="c0"><span class="c5">the generalized pigeonhole principle</span><span>&nbsp;- when N objects are placed into k boxes, there must be a box containing at least &lceil;N/k&rceil; objects</span></li><li class="c0"><span>P(n,r)=n!/(n-r)!</span></li><li class="c0"><span>C(n,r)=(n over r)=n!/[r!(n-r)!]</span></li><li class="c0"><span class="c5">pascal&rsquo;s identity</span><span>&nbsp;-</span><span>&nbsp;(n+1 o k)= (n o k-1)+(n &nbsp;o k)</span></li><li class="c0"><span class="c5">the binomial theorem</span><span>&nbsp;- (x+y)^n = &sum;_(k=0)^n (n o k)(x^(n-k))(y^k)</span></li><li class="c0"><span>there are n^r r-permutations of a set with n elements when repetition is allowed</span></li><li class="c0"><span>there are C(n+r-1, r) r-combinations of a set with n elements when repetition is allowed</span></li><li class="c0"><span>there are n!/(n1!n2!...nk!) permutations of an objects of k types where there are indistinguishable objects of type I for I = 1,2,3&hellip;k</span></li><li class="c0"><span class="c5">sample space</span><span>&nbsp;- the set of possible outcomes of an experiment</span></li><li class="c0"><span>event: a subset of the sample space of an experiment</span></li><li class="c0"><span>probability of an event(Laplace&rsquo;s definition): the number of successful outcomes of this event divided by the number of possible outcomes</span></li><li class="c0"><span>probability distribution: a function p from the set of all outcomes of a sample space S for which 0 &lt;= p(xi) for I = &nbsp;1,2&hellip;n and the sum from I=1 to n of p(xi)=1, where xi&hellip;xn are the possible outcomes</span></li><li class="c0"><span class="c5">probability of an event E</span><span>&nbsp;- the sum of the probabilities of the outcomes in E</span></li><li class="c0"><span class="c5">conditional probability of E given F</span><span>&nbsp;- p(E|F) - the ratio p(E intersect F)/p(F)</span></li><li class="c0"><span class="c5">independent events</span><span>&nbsp;- events E and F such that p(E intersect F)=p(E)p(F)</span></li><li class="c0"><span class="c5">pairwise independent events</span><span>&nbsp;- &nbsp;events E1, E2,&hellip;En such that p(Ei intersect Ej)=p(Ei)p(Ej) for all pairs of integers i and j with 1&lt;=j&lt;k&lt;=n</span></li><li class="c0"><span class="c5">mutually independent events</span><span>&nbsp;- events E1, E2,&hellip;En such that the probability of &nbsp;intersection of &nbsp;the events up to Em is the product of the probability of each individually where m&lt;=n and m&gt;=2</span></li><li class="c0"><span class="c5">random variable</span><span>&nbsp;- a function that assigns a real number to each possible outcome of an expression</span></li><li class="c0"><span class="c5">distribution of a random variable X</span><span>&nbsp;- the set of pairs(r, p(X=r)) for r &isin; X(s)</span></li><li class="c0"><span class="c5">uniform distribution</span><span>&nbsp;- the assignment of equal probabilities to the elements of a finite set</span></li><li class="c0"><span class="c5">expected value of a random variable </span><span>- the weighted average of a random variable, with values of the random variable weighted by the probability of outcomes, that is E(X)=&sum;_(s&isin;S)p(s)X(s)</span></li><li class="c0"><span class="c5">geometric distribution</span><span>&nbsp;- the distribution of a random variable X such that p(X=k)-(1-p)^(k-1) for k = 1,2,&hellip; for some real number p with 0&lt;=p&lt;=1</span></li><li class="c0"><span class="c5">independent random variables</span><span>&nbsp;- random variables X and Y such that p(X=r1 and Y=r2)=p(X=r1)p(Y=r2) for all real numbers r1 and r2</span></li><li class="c0"><span class="c5">variance of a random variable X</span><span>&nbsp;- the weighted average of the square of the difference between the value of K and its expected value E(X), with weights given by the probability of outcomes, that is, V(X)=&sum;_(s&isin;S)(S(s)-E(X))^2p(s)</span></li><li class="c0"><span class="c5">standard deviation of a &nbsp; variable X</span><span>&nbsp;- the square root of the variance of X, that is &sigma;(X)=sqrt(V(X))</span></li><li class="c0"><span class="c5">Bernoulli trial </span><span>- an experiment with two possible outcomes</span></li><li class="c0"><span class="c5">probabilistic(or Monte Carlo) algorithm</span><span>&nbsp;- an algorithm in which random choices are made are one or more steps</span></li><li class="c0"><span class="c5">probabilistic method</span><span>&nbsp;- a technique for proving the existence of objects in a set with certain properties that proceeds by assigning probabilities to objects and showing that the probability that an object has these probabilities is positive. the probability of exactly k successes when n independent Bernoulli trials are carried out equals C(n,k)p^(k)q^(n-k), where p is the probability of success and q=p-1 is the probability of failure</span></li><li class="c0"><span class="c5">bayes&rsquo; theorem</span><span>&nbsp;- if E and F are events from a sample space S such that p(E) &ne; 0 and p(F) &ne; 0, then P(E|F) = P(F|E) * P(E) / P(F)</span></li><li class="c0"><span class="c5">linearity of expectations</span><span>&nbsp;- E(x1+x2+&hellip;+xn)=E(X1)+ E(x2)+&hellip;+ E(xn), if x1, x2, &hellip;, xn are random variables</span></li><li class="c0"><span>if X and y are independent random variables, then E(XY)=E(X)E(Y)</span></li><li class="c0"><span class="c5">beinayme&rsquo;s formula</span><span>&nbsp;- if x1, x2, &hellip;, xn are independent random variables, then V(x1+x2+&hellip;+xn)= V(x1)+V(x2)+&hellip;+V(xn)</span></li><li class="c0"><span class="c5">chebyshev&rsquo;s inequality</span><span>&nbsp;- p(|X(s)-E(X)|&gt;=r) &lt;= V(X)/r^2 where X is a random variable with probability function p and r is a positive real number</span></li></ol><hr><p class="c7"><span></span></p><h1 class="c9"><a name="h.aevbh03k9s2u"></a><span>Algorithms (lecture 11)</span></h1><ol class="c8" start="1"><li class="c0"><span>&nbsp;Algorithm - finite sequence of precise instructions to perform a computation</span></li></ol><h2 class="c9"><a name="h.h7hulbkku891"></a><span>Searches</span></h2><ol class="c8" start="2"><li class="c0"><span>Linear search - start at the front, brute-force until you find it. Worst case: </span><span class="c4">O</span><span>(</span><span class="c4">n</span><span>)</span></li><li class="c0"><span>Binary search - start at the middle of a sorted list. If you&rsquo;re too high, search the left half of the list. If you&rsquo;re too low, search the right half of the list. Worst case: </span><span class="c4">O</span><span>(log(</span><span class="c4">n</span><span>))</span></li></ol><h2 class="c9"><a name="h.6icvczmib36u"></a><span>Sorts</span></h2><ol class="c8" start="4"><li class="c0"><span class="c6"><a class="c10" href="http://www.youtube.com/watch?v=P00xJgWzz2c">Bubble sort</a></span><span>&nbsp;- switch adjacent values over and over again until you don&rsquo;t switch no more. Another way to think about bubble sort is: find the biggest element and put it at the end, then look for the second biggest element and put it one from the end and so on.<br>Worst case </span><span class="c4">O</span><span>(</span><span class="c4">n</span><span class="c12">2</span><span>), best case </span><span class="c4">O</span><span>(</span><span class="c4">n</span><span>). </span></li><li class="c0"><span>Insertion sort - place each element from the unsorted list to the correct place in the sorted list. Worst case </span><span class="c4">O</span><span>(</span><span class="c4">n</span><span class="c12">2</span><span>), best case </span><span class="c4">O</span><span>(</span><span class="c4">n</span><span>).</span></li></ol><h2 class="c9"><a name="h.if58rrapk5cb"></a><span>Halting problem</span></h2><ol class="c8" start="1"><li class="c0"><span>Procedure that cannot be solved with an algorithm. </span></li><li class="c0"><span>H(P, I) defines H as a procedure which takes in two arguments. P is a program and I is arguments to P. H returns true if P will halt with arguments I or false if P loops for ever</span></li><li class="c0"><span>Proof that H(P, I) cannot be solved with a procedure: Define K(P) as halting if H(P,P) ( a program can take itself as an argument) returns false (run for ever). K(P) runs for ever if H(P,P) returns true (halting).</span></li><li class="c0"><span>If we pass K itself K(K). If K halts then H(K,K) would&rsquo;ve said the K would run for ever and also if K runs forever H(K,K) would&rsquo;ve said that K halts. Thus creating a contradiction</span></li></ol><h1 class="c9"><a name="h.b7hlcsa3iwjm"></a><span>Costs of a computation (lecture 12)</span></h1><h2 class="c9"><a name="h.aixo5vaih2m5"></a><span>Big O, Big Omega, Big Theta</span></h2><ol class="c8" start="1"><li class="c0"><span>Big O: upper bound on the growth of the function</span></li></ol><ol class="c2" start="1"><li class="c1"><span class="c4">f</span><span>(</span><span class="c4">x</span><span>) = </span><span class="c4">O</span><span>(</span><span class="c4">g</span><span>(</span><span class="c4">x</span><span>)) therefore </span><span class="c4">C</span><span>*</span><span class="c4">g</span><span>(</span><span class="c4">x</span><span>) &ge; </span><span class="c4">f</span><span>(</span><span class="c4">x</span><span>) &nbsp;</span></li></ol><ol class="c8" start="2"><li class="c0"><span>Big Omega: lower bound on the growth of a function</span></li></ol><ol class="c2" start="1"><li class="c1"><span class="c4">f</span><span>(</span><span class="c4">x</span><span>) = &Omega;(</span><span class="c4">g</span><span>(</span><span class="c4">x</span><span>)) therefore </span><span class="c4">C</span><span>*</span><span class="c4">g</span><span>(</span><span class="c4">x</span><span>) &le; </span><span class="c4">f</span><span>(</span><span class="c4">x</span><span>)</span></li></ol><ol class="c8" start="3"><li class="c0"><span>Big Theta: a function that describes the same growth order as function you are observing</span></li></ol><ol class="c2" start="1"><li class="c1"><span class="c4">f</span><span>(</span><span class="c4">x</span><span>) = &#1012;(</span><span class="c4">g</span><span>(</span><span class="c4">x</span><span>)) therefore </span><span class="c4">C</span><span class="c11">1</span><span>*</span><span class="c4">f</span><span>(</span><span class="c4">x</span><span>) = O(</span><span class="c4">g</span><span>(</span><span class="c4">x</span><span>)) &amp; C</span><span class="c11">2</span><span>*</span><span class="c4">f</span><span>(</span><span class="c4">x</span><span>) = &Omega;(</span><span class="c4">g</span><span>(</span><span class="c4">x</span><span>))</span></li></ol><ol class="c8" start="4"><li class="c0"><span>Only matters as the function progresses to infinity, you are allowed to say for all </span><span class="c4">x</span><span>&nbsp;&gt; </span><span class="c4">k</span></li><li class="c0"><span>When disproving all big O or big &Omega;, show no such </span><span class="c4">k</span><span>&nbsp;or </span><span class="c4">C</span><span>&nbsp;values exist</span></li><li class="c0"><span>Algebra relations</span></li></ol><ol class="c2" start="1"><li class="c1"><span>Addition</span></li></ol><ol class="c3" start="1"><li class="c9 c14"><span class="c4">f</span><span>(</span><span class="c4">x</span><span>) is </span><span class="c4">O</span><span>(</span><span class="c4">g</span><span>(</span><span class="c4">x</span><span>)) &amp; </span><span class="c4">e</span><span>(</span><span class="c4">x</span><span>) is </span><span class="c4">O</span><span>(</span><span class="c4">h</span><span>(</span><span class="c4">x</span><span>)) then (</span><span class="c4">f</span><span>&nbsp;+ </span><span class="c4">e</span><span>) is </span><span class="c4">O</span><span>(max( </span><span class="c4">g</span><span>(</span><span class="c4">x</span><span>), </span><span class="c4">h</span><span>(</span><span class="c4">x</span><span>) ))</span></li></ol><ol class="c2" start="2"><li class="c1"><span>Scalar multiplication</span></li></ol><ol class="c3" start="1"><li class="c14 c9"><span>When f(x) is O(g(x)) and there is a constant k then k*f(x) is O(g(x))</span></li></ol><ol class="c2" start="3"><li class="c1"><span>Product</span></li></ol><ol class="c3" start="1"><li class="c14 c9"><span>f(x) is O(g(x)) &amp; e(x) is O(h(x)) then (f*e) is O( g*h ) </span></li></ol><ol class="c8" start="7"><li class="c0"><span>To determine a function&#39;s big O you must find the sum equivalent value and determining what binds it&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></li><li class="c0"><span>If the limit exists and is finite as x goes to infinity of f(x)/g(x) then f(x) &isin; O(g(x))</span></li></ol><ol class="c2" start="4"><li class="c1"><span>if the limit is infinite, then f(x) &notin; O(g(x))</span></li></ol><h2 class="c9"><a name="h.bgrzeprm4gdn"></a><span>Tractability</span></h2><ol class="c8" start="1"><li class="c0"><span>A problem is considered tractable if the algorithm to solve the problem has polynomial or less complexity</span></li><li class="c0"><span>A problem is considered intractable if the algorithm to solve the problem had exponential or higher complexity</span></li><li class="c0"><span>A problem is considered unsolvable if no algorithm solves the problem. (ie: The Halting Problem)</span></li></ol><h1 class="c9"><a name="h.fuzll8bwum50"></a><span>Counting (lecture 13)</span></h1><h2 class="c9"><a name="h.uxwlb5bapam9"></a><span>Inclusion-Exclusion</span></h2><ol class="c8" start="1"><li class="c0"><span>|A &cup; B| = |A| + |B| - |A &cap; B|</span></li><li class="c0"><span>|A &cup; B &cup; C| = |A| + |B| + |C| - |A &cap; B| - |A &cap; C| - |B &cap; C| + |A &cap; B &cap; C|</span></li></ol><h2 class="c9"><a name="h.yfyvopotlx7"></a><span>Cartesian Product</span></h2><ol class="c8" start="1"><li class="c0"><span>S = S1 x S2 x ... x Sn</span></li><li class="c0"><span>|S| = |S1| |S2| ... |Sn| (whether disjoint or non-disjoint)</span></li><li class="c0"><span>// Composite: not prime</span></li></ol><h2 class="c9"><a name="h.rvrrge193tbi"></a><span>Pigeonhole principle</span></h2><ol class="c8" start="1"><li class="c0"><span>If </span><span class="c4">N</span><span>&nbsp;pigeons nest in </span><span class="c4">k</span><span>&nbsp;&lt; </span><span class="c4">N</span><span>&nbsp;holes, then at least one hole will have two or more pigeons.</span></li><li class="c0"><span>Generalized Pigeonhole Principle</span></li></ol><ol class="c2" start="1"><li class="c1"><span>If N objects are placed into k boxes, then there is at least one box containing at least CEIL(</span><span class="c4">N</span><span>&nbsp;/ </span><span class="c4">k</span><span>) objects.</span></li></ol><ol class="c8" start="3"><li class="c0"><span>Purified Pigeonhole Principle (EWD)</span></li></ol><ol class="c2" start="1"><li class="c1"><span>For a nonempty, finite bag of numbers, [bag allows repetitions] the maximum value is at lea</span></li><li class="c1"><span>st the average value.</span></li></ol><h1 class="c9"><a name="h.gzdvx5bom1t0"></a><span>Binomial Theorem (lecture 14 &amp; 15)</span></h1><ol class="c8" start="1"><li class="c0"><span>Definition</span></li></ol><ol class="c2" start="1"><li class="c1"><span>n </span><span class="c21">&ge;</span><span>&nbsp;0, (x + y)</span><span class="c12">n</span><span>&nbsp;= </span><img src="images/image00.png"></li></ol><ol class="c8" start="2"><li class="c0"><span>Similar to the </span><span class="c6"><a class="c10" href="http://en.wikipedia.org/wiki/Multinomial_theorem">multinomial theorem</a></span><span>, which expands it out to an arbitrary </span><span class="c4">m</span><span>&nbsp;elements</span></li></ol><h1 class="c9"><a name="h.rxh66hq1oxjz"></a><span>Combinations &amp; </span><span>Permutations</span></h1><h2 class="c9"><a name="h.ab56loh4equs"></a><span>Combinations/Choose (aka binomial coefficient): order doesn&rsquo;t matter</span></h2><ol class="c8" start="1"><li class="c0"><span class="c4">C</span><span>(</span><span class="c4">n</span><span>, </span><span class="c4">k</span><span>)</span></li><li class="c0"><span>The number of ways to choose k objects from n objects if </span><span class="c4">order doesn&#39;t matter</span></li><li class="c0"><span>For example, you want to pick a team of 5 from 20 people -- order doesn&#39;t matter, so it&#39;s </span><span class="c4">C</span><span>(20, 5)</span></li><li class="c0"><span class="c4">C</span><span>(</span><span class="c4">n</span><span>, </span><span class="c4">k</span><span>) = </span><img src="images/image01.png"><span>&nbsp;(and </span><span class="c6"><a class="c10" href="http://en.wikipedia.org/wiki/Binomial_coefficient#Factorial_formula">you can see it prettier on Wikipedia</a></span><span>)</span></li></ol><h2 class="c9"><a name="h.u5eo5dth9ro2"></a><span>Permutations: order matters</span></h2><ol class="c8" start="1"><li class="c0"><span class="c4">P</span><span>(</span><span class="c4">n</span><span>, </span><span class="c4">k</span><span>)</span></li><li class="c0"><span>The number of ways to choose (pick) k objects from n objects if </span><span class="c4">order matters</span></li><li class="c0"><span>For example, if you want to pick the top 10 songs from 200, order matters, so it&#39;s </span><span class="c4">P</span><span>(200, 10)</span></li><li class="c0"><span class="c4">P</span><span>(</span><span class="c4">n</span><span>, </span><span class="c4">k</span><span>) = </span><span class="c4">n</span><span>! / (</span><span class="c4">n</span><span>&nbsp;- </span><span class="c4">k</span><span>)! (and </span><span class="c6"><a class="c10" href="http://en.wikipedia.org/wiki/Permutations#In_combinatorics">you can see it prettier on Wikipedia</a></span><span>)</span></li></ol><h1 class="c9"><a name="h.wzlgvg728sem"></a><span>Stirling numbers of the second kind</span></h1><ol class="c8" start="1"><li class="c0"><span class="c4">S</span><span>(</span><span class="c4">n</span><span>, </span><span class="c4">k</span><span>) = the amount of ways to split n distinguishable objects into k indistinguishable bins</span></li><li class="c0"><span>For example, if you have 4 employees and want to put them into 2 offices (and you can&#39;t distinguish the offices), that&#39;s </span><span class="c4">S</span><span>(</span><span class="c4">4</span><span>, </span><span class="c4">2</span><span>) = 7 different ways to do this &nbsp;</span></li><li class="c0"><span>There&#39;s </span><span class="c6"><a class="c10" href="http://en.wikipedia.org/wiki/Stirling_numbers_of_the_second_kind#Definition">an ugly formula</a></span><span>&nbsp;which you might wanna write down<br></span><img height="60" src="images/image04.png" width="247"></li><li class="c0"><span>It&#39;s usually best to </span><span class="c6"><a class="c10" href="http://www.google.com/url?q=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FStirling_numbers_of_the_second_kind%23Table_of_values&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFG9a-hKLK_Kg8mSG2wnb_c0iJBjg">use a chart</a></span><span>, which you might also want to write down</span></li></ol><h1 class="c9"><a name="h.7w20letd4a0a"></a><span>Probability</span></h1><ol class="c8" start="1"><li class="c0"><span>Starts with a defined sample space S and an event E in that sample space</span></li></ol><ol class="c2" start="1"><li class="c1"><span>probability of E is P(E) = |E| / |S|</span></li><li class="c1"><span>0 &le; P(E) &le; 1</span></li><li class="c1"><span>P(not E) = 1 - P(E)</span></li><li class="c1"><span>P(E</span><span class="c11">1</span><span>&nbsp;&cup; E</span><span class="c11">2</span><span>) = P(E</span><span class="c11">1</span><span>) + P(E</span><span class="c11">2</span><span>) - P(E</span><span class="c11">1</span><span>&nbsp;&cap; E</span><span class="c11">2</span><span>)</span></li></ol><ol class="c8" start="2"><li class="c0"><span>Method for probability</span></li></ol><ol class="c2" start="1"><li class="c1"><span>Find the sample space</span></li><li class="c1"><span>Define events of interest</span></li><li class="c1"><span>Determine outcome probabilities</span></li><li class="c1"><span>Compute event probabilities</span></li></ol><h1 class="c9"><a name="h.a2edd996p535"></a><span>Bayes&#39; Theorem</span></h1><ol class="c8" start="1"><li class="c0"><span>P(E|F) = the probability of E, given F</span></li><li class="c0"><img src="images/image02.png"></li></ol><ol class="c2" start="1"><li class="c1"><span>G is the complement of E</span></li></ol><ol class="c8" start="3"><li class="c0"><span>More generally, </span><img src="images/image03.png"><span>. P(F) may be given or easier to determine.</span></li><li class="c0"><span>If you have P(F) = &frac12;, then the you can use a simpler version: P(E|F) = P(F|E) / [ P(F|E) + P(F|E complement)]</span></li></ol><p class="c7"><span></span></p><h1 class="c9"><a name="h.s8i6f2c06gq5"></a><span>Expectation and variance</span></h1><ol class="c8" start="1"><li class="c0"><span>Expected value = E(x)</span></li><li class="c0"><span>(X(s) is a random variable)</span></li></ol><ol class="c2" start="1"><li class="c1"><span>E(x) = &Sigma; P(s</span><span class="c11">i</span><span>)X(s</span><span class="c11">i</span><span>) </span></li><li class="c1"><span>Expectations are linear, whether or not X</span><span class="c11">i</span><span>&nbsp;are independent</span></li></ol><ol class="c3" start="1"><li class="c14 c9"><span>Useful in proving basic propositions</span></li><li class="c14 c9"><span>E(X_1 + X_2) = E(X_1) + E(X_2)</span></li><li class="c14 c9"><span>E(aX + b) = aE(X) + b</span></li></ol><ol class="c2" start="3"><li class="c1"><span>Total Expectations</span></li></ol><ol class="c3" start="1"><li class="c14 c9"><span>E(R) = &Sigma;E(R|A</span><span class="c11">i</span><span>)X(A</span><span class="c11">i</span><span>)</span></li></ol><ol class="c8" start="3"><li class="c0"><span>Deviation is X(s) - E(x) however this is not very useful and is often squared</span></li><li class="c0"><span>Variance</span></li></ol><ol class="c2" start="4"><li class="c1"><span class="c4">V</span><span>(</span><span class="c4">x</span><span>) = &Sigma;_s&isin;S (</span><span class="c4">X</span><span>(</span><span class="c4">s</span><span>) - </span><span class="c4">E</span><span>(</span><span class="c4">x</span><span>))</span><span class="c12">2 </span><span class="c4">p</span><span>(</span><span class="c4">s</span><span>)</span></li><li class="c1"><span>Standard deviation = </span><span class="c20">&radic;</span><span class="c4">V</span><span>(</span><span class="c4">x</span><span>)</span></li></ol><p class="c7"><span></span></p><hr><p class="c7"><span></span></p><h1 class="c9"><a name="h.7sm4nm8ptdxy"></a><span>Example problems</span></h1><p class="c9"><span class="c5">Q: Monty Hall problem: 3 doors, one with a car and two with goats. You choose one without opening it and Monty shows you behind a different door, one that has a goat. Should you change your door?</span></p><p class="c9"><span>A: Yes. Why? When you choose first, your chances are &#8531;, and so your chances of being </span><span class="c4">wrong</span><span>&nbsp;are &#8532;. But when Monty eliminates a door the chance that the door you picked is still &#8531;, and so 1 - &#8531;= &#8532; is distributed across the remaining doors, in this case 1 door. So &#8532; is the probability that the remaining door has the prize.</span></p><p class="c7"><span></span></p><p class="c9"><span class="c5">Q: Hat check problem: You check your hat in, but the hat man forgot to put numbers on the hats. What is the expected number of people that get their own hat back?</span></p><p class="c9"><span>A: 1. Why? Expectations are linear. X = X1 + X2 + X3 + X4 + X5.... + Xn. E(Xi) = 1/n. You can add up the chances of each person getting their hat back to ge the expected number. Summation of (1/n) from i = 1 to n, which is 1. [For example 3 people... 1/3 + 1/3 + 1/3 = 1]</span></p><p class="c7"><span></span></p><p class="c9"><span class="c5">Q: If you throw 2 dice, what is the expected number of throws before you get snake eyes.</span></p><p class="c9"><span>A: 36. Why? The total number of ways 2 die can land is 36, and snake eyes only appears in 1 of those 36. So 1/p = 36, p = 1/36</span></p><p class="c7"><span></span></p><p class="c9"><span class="c5">Q: The SAT has a mean of 500 and standard deviation of 100, what does Chebyshevs tell us about getting a score greater than 700?</span></p><p class="c9"><span>A: 1/8. Why? &nbsp;P(SAT) &gt;= 700 = &frac12;P(SAT &gt;= 700 or &lt;= 300) = &frac12; V(X)/r^2 = &frac12; * (100^2)/(200^2) = 1/8 &nbsp; The &frac12; is because we want just scores &gt; 700 not less than 300. 100 = the standard deviation and </span><span>the 200 comes from X(s) - E(X) which here is 700 - 500.</span></p><p class="c7"><span class="c15"></span></p><p class="c9"><span class="c5">Q: a) What is the probability that two people chosen at random were born during the same month of the year? b) What is the probability that in a group of n people chosen at random that there are at least two born in the same month of the year? c) How many people chosen at random are needed to make the probability greater than 1/2 that there are two people born in the same year. </span></p><p class="c9"><span>A: a) 1/12. There are 12 months in the year, and being born is an independent event. That is, the probability that you were born in a certain month doesn&rsquo;t depend on the probability that another individual was born in that same month. Since there is only one way for two people to be born on the same month the probability is 1/12. </span></p><p class="c9"><span>b) Same as birthday problem. </span></p><p class="c9"><span>c) 5. ???</span></p><p class="c7"><span class="c5"></span></p><p class="c9"><span class="c5">Q: a) Prove that S(n,n-1) = C(n,2) for n &gt;= 2 b) Prove that S(n,2) = 2</span><span class="c12 c5">n-1</span><span class="c5">&nbsp;- 1, n &gt;= 2</span></p><p class="c9"><span class="c5">A:</span><span>&nbsp;a) By the Pigeonhole Principle, a placement must have 2 balls in the same urn, while by the requirement that no urn is empty, each of the other urns contains precisely 1 ball. Thus a 2placement uniquely determine a pair of balls. Conversely a pair of balls uniquely determine a placement in which the two balls in the pair are in the same urn and others in their other urn. Thus there are C(n,2) placements.</span></p><p class="c9"><span>b) Each non-empty proper subset A subset of n uniquely determines a placement with A in one urn and B = not A in the other urn. For each placement precisely 2 proper subsets A and not A would give the same placement (by setting A to be the set of balls of either urn). Thus the number of placements is (2</span><span class="c12">n</span><span>- 2)/2 = 2</span><span class="c12">n-1</span><span>&nbsp;- 1</span></p><p class="c7"><span></span></p><p class="c9"><span class="c5">Q: a) Suppose we have a training set of 10,000 spam messages and 5000 non spam messages. The word enhancement appears in 1500 spam messages and 20 non spam messages. Starting with a the uniform prior what is the posterior probability that a message is spam given that it contains the word &ldquo;enhancement&rdquo;.</span></p><p class="c9"><span class="c5">A: </span><span>a) p( enhancement | Spam ) = 1500/10000 = .15, p(enhancement | not Spam) = 20/5000 = .004, p(Spam | enhancement) = (.15*.4)/(.15*.5 + .004*.5) = .150/.154 = .974 or 97.4%</span></p><p class="c7"><span></span></p><p class="c9"><span>Know how to do a Bayes&rsquo; theorem problem involving spam filters (1, 2, or 3 filter words). </span></p><p class="c7"><span></span></p><p class="c9"><span>More examples can be found on the homework problems, which might be worth copying onto your formulas sheet.</span></p><h1 class="c9"><a name="h.igh2e2ur7ces"></a><span>Ideas for things to copy down</span></h1><ol class="c8" start="1"><li class="c0"><span>Summation formulas in chapter 2<br></span><img height="296" src="images/image05.png" width="262"></li><li class="c0"><span>Best/worst cases for algorithms</span></li><li class="c0"><span>Formulas </span></li><li class="c0"><span>Table of Stirling numbers of the second kind</span></li><li class="c0"><span>Homework problems</span></li><li class="c0"><span>Placing </span><span class="c4">n</span><span>&nbsp;balls into </span><span class="c4">m</span><span>&nbsp;urns formulas from review</span></li><li class="c0"><span class="c6"><a class="c10" href="https://docs.google.com/document/d/1F-tKiuFBBBhBPd1NSKfCRArxBeyot4MItRF4A--a3Ik/edit">Typed up vocab</a></span></li></ol><p class="c7"><span></span></p><p class="c9"><img height="181" src="images/image07.png" width="310"><img height="305" src="images/image06.png" width="281"></p><p class="c7"><span></span></p><p class="c9"><span>This is awesome, thank you!</span></p><p class="c7"><span></span></p><p class="c9"><span>how many other people think this exam is gunna suck?</span></p><p class="c9"><span class="c15">Suck | Not Suck| Double Suck Sundae</span></p><p class="c9"><span>&nbsp; &nbsp;10 &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; | &nbsp; &nbsp; &nbsp;1</span></p><p class="c7"><span></span></p></body></html>